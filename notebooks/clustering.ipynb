{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import glob\n",
    "\n",
    "import operator\n",
    "from uuid import uuid4\n",
    "\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "from copy import deepcopy\n",
    "\n",
    "import umap.umap_ as umap\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import LlamaCppEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3cc08a3ba8cebb2c",
   "metadata": {},
   "source": [
    "## Model laden"
   ]
  },
  {
   "cell_type": "code",
   "id": "54b77828361e295d",
   "metadata": {},
   "source": [
    "embeddings_model = LlamaCppEmbeddings(\n",
    "    model_path=\"models/mxbai-embed-large-v1.Q8_0.gguf\",\n",
    "    verbose=False,\n",
    "    n_gpu_layers=-1 # Set to 0 for only cpu\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2a1bc204dcf5305d",
   "metadata": {},
   "source": [
    "## Dateien einlesen\n",
    "\n",
    "Wir lesen die Dateien so ein, dass jede Datei ein logisches Dokument ist und nicht wie sonst ein Dokument pro Seite"
   ]
  },
  {
   "cell_type": "code",
   "id": "6bab907d05a95952",
   "metadata": {},
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "pdf_paths = glob.glob(\"test-data/**/*.pdf\", recursive=True)\n",
    "docs_paths = glob.glob(\"test-data/**/*.docx\", recursive=True)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for path in pdf_paths:\n",
    "    text = \"\"\n",
    "    last_meta = {}\n",
    "    loader = PyPDFLoader(path)\n",
    "    async for page in loader.alazy_load():\n",
    "        text += \"\\n\\n\"\n",
    "        text += page.page_content.lower()\n",
    "        last_meta = page.metadata\n",
    "\n",
    "    documents.append(Document(text, metadata={\n",
    "        \"source\": last_meta[\"source\"],\n",
    "    }))\n",
    "\n",
    "\n",
    "for path in docs_paths:\n",
    "    text = \"\"\n",
    "    last_meta = {}\n",
    "    loader = Docx2txtLoader(path)\n",
    "    async for page in loader.alazy_load():\n",
    "        text += \"\\n\\n\"\n",
    "        text += page.page_content.lower()\n",
    "        last_meta = page.metadata\n",
    "\n",
    "    documents.append(Document(text, metadata={\n",
    "        \"source\": last_meta[\"source\"],\n",
    "    }))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca5f2aaad3139473",
   "metadata": {},
   "source": [
    "## Text splitten\n",
    "Overlap, damit die Embeddings der Chunks näher beieinander sind"
   ]
  },
  {
   "cell_type": "code",
   "id": "ad6e22ace09b2672",
   "metadata": {},
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=150)\n",
    "splits = text_splitter.split_documents(documents)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9cdd6927961f77d4",
   "metadata": {},
   "source": [
    "## Embeddings erstellen und in DB speichern"
   ]
  },
  {
   "cell_type": "code",
   "id": "260c865fc40a612b",
   "metadata": {},
   "source": [
    "vectorstore = Chroma(\n",
    "    embedding_function=embeddings_model,\n",
    ")\n",
    "\n",
    "emb_ids = [str(uuid4()) for x in splits]\n",
    "\n",
    "for i in range(len(splits)):\n",
    "    splits[i].metadata[\"id\"] = emb_ids[i]\n",
    "\n",
    "_ = vectorstore.add_documents(documents=splits, ids=emb_ids)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1cd85adcbc1acb0a",
   "metadata": {},
   "source": [
    "## Alle Embeddings aus der DB lesen"
   ]
  },
  {
   "cell_type": "code",
   "id": "598ed31604e068de",
   "metadata": {},
   "source": [
    "embedding_entries = vectorstore.get(include=[\"metadatas\", \"embeddings\"])\n",
    "\n",
    "embeddings = embedding_entries.get(\"embeddings\")\n",
    "ids = embedding_entries.get(\"ids\")\n",
    "sources = [x[\"source\"] for x in  embedding_entries.get(\"metadatas\")]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c1530ffc94e76815",
   "metadata": {},
   "source": [
    "## Embeddings clustern\n",
    "\n",
    "Die Embeddings bekommen ein Label von -1 bis ..., welches die Cluster ID darstellt <br>\n",
    "Die ID -1 sagt aus, dass es sich um \"Noisy\" Punkte handelt"
   ]
  },
  {
   "cell_type": "code",
   "id": "8d11a16ed17480cd",
   "metadata": {},
   "source": "hdb = hdbscan.HDBSCAN(min_samples=4, min_cluster_size=10, metric='euclidean', cluster_selection_epsilon=0.2).fit(embeddings)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dd092ed1a6df695",
   "metadata": {},
   "source": [
    "## Datensatz mit Clustern aufbauen"
   ]
  },
  {
   "cell_type": "code",
   "id": "79a0a620fd1cdd19",
   "metadata": {},
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"id\"] = ids\n",
    "df[\"source\"] = sources\n",
    "df[\"cluster\"] = hdb.labels_.astype(int)\n",
    "\n",
    "#Kopieren für rohen Datensatz als Vergleich\n",
    "df_raw = deepcopy(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6d70551f4def5900",
   "metadata": {},
   "source": [
    "## Rohes Cluster darstellen"
   ]
  },
  {
   "cell_type": "code",
   "id": "c88b071d0da27f6d",
   "metadata": {},
   "source": [
    "u_file = umap.UMAP(n_components=2, random_state=42, n_neighbors=80, min_dist=0.1)\n",
    "\n",
    "df_umap_file = (\n",
    "    pd.DataFrame(u_file.fit_transform(np.array(embeddings)), columns=['x', 'y'])\n",
    "    .assign(source=lambda df_: df_raw[\"source\"].astype(str))\n",
    ")\n",
    "\n",
    "fig = px.scatter(df_umap_file, x='x', y='y', color=\"source\",  title='Dateien').update_traces(dict(marker_line_width=0.5, marker_line_color=\"black\"))\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "238358229519acb1",
   "metadata": {},
   "source": [
    "u_raw = umap.UMAP(n_components=2, random_state=42, n_neighbors=80, min_dist=0.1)\n",
    "\n",
    "df_umap_raw = (\n",
    "    pd.DataFrame(u_raw.fit_transform(np.array(embeddings)), columns=['x', 'y'])\n",
    "    .assign(cluster=lambda df_: df_raw[\"cluster\"].astype(str))\n",
    "    #.query('cluster != \"-1\"') # Noisy Punkte filtern\n",
    "    .sort_values(by='cluster')\n",
    ")\n",
    "\n",
    "fig = px.scatter(df_umap_raw, x='x', y='y', color='cluster', title='Rohe Cluster').update_traces(dict(marker_line_width=0.5, marker_line_color=\"black\"))\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "efc84906de99a5c6",
   "metadata": {},
   "source": [
    "df_raw"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "231e1ee55cf87e7f",
   "metadata": {},
   "source": [
    "## Cluster nachbearbeiten\n",
    "\n",
    "Alle Embeddings einer Datei, sollen im gleichen Cluster sein. Dafür setzen wir die Cluster ID auf die ID, welche die Mehrheit der Datenpunkte bekommen hat. <br>\n",
    "Bei der Mehrheitssuche wird die ID -1 ignoriert, das heißt am Ende gibt es keine Noisy Punkte mehr\n",
    "\n",
    "<br>\n",
    "\n",
    "#### TODO\n",
    "Den Fall berücksichtigen, wenn eine Datei nur aus Noisy Punkten besteht\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e20ec39fcd6c1773",
   "metadata": {},
   "source": [
    "for source in df[\"source\"].unique():\n",
    "    assigned_clusters: dict[int, int] = {}\n",
    "\n",
    "    for row in df.loc[df['source'] == source].itertuples():\n",
    "        assigned_clusters[row.cluster] = assigned_clusters.get(row.cluster, 0) + 1\n",
    "\n",
    "    if assigned_clusters.get(-1):\n",
    "        assigned_clusters[-1] = 0\n",
    "\n",
    "    target_cluster = max(assigned_clusters.items(), key=operator.itemgetter(1))[0]\n",
    "    df.loc[df['source'] == source, \"cluster\"] = target_cluster\n",
    "\n",
    "last_cluster_id = max(df[\"cluster\"].unique())\n",
    "for source in df[\"source\"].unique():\n",
    "    if df.loc[df['source'] == source, \"cluster\"].iloc[0] != -1:\n",
    "        continue\n",
    "    last_cluster_id += 1\n",
    "    df.loc[df['source'] == source, \"cluster\"] = last_cluster_id"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cab92adb3de42257",
   "metadata": {},
   "source": [
    "## Fertiges Cluster"
   ]
  },
  {
   "cell_type": "code",
   "id": "816fe8c2ecc7b61e",
   "metadata": {},
   "source": [
    "u = umap.UMAP(n_components=2, random_state=42, n_neighbors=80, min_dist=0.1)\n",
    "\n",
    "df_umap = (\n",
    "    pd.DataFrame(u.fit_transform(np.array(embeddings)), columns=['x', 'y'])\n",
    "    .assign(cluster=lambda df_: df[\"cluster\"].astype(str))\n",
    "    #.query('cluster != \"-1\"')\n",
    "    .sort_values(by='cluster')\n",
    ")\n",
    "\n",
    "fig = px.scatter(df_umap, x='x', y='y', color='cluster', title='Fertiges clustering').update_traces(dict(marker_line_width=0.5, marker_line_color=\"black\"))\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "faddde210ec231a2",
   "metadata": {},
   "source": [
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f23a9ceb0a8086ff",
   "metadata": {},
   "source": [
    "## Finde Cluster auf Basis von Text\n",
    "\n",
    "Bei der Textsuche bekommt man bessere Ergebnisse, wenn man einen Prompt zur Suche benutz als nur mit einem Keyword"
   ]
  },
  {
   "cell_type": "code",
   "id": "d1af7b8dacc60d46",
   "metadata": {},
   "source": [
    "search_term = \"3D Druck\"\n",
    "\n",
    "clusters = []\n",
    "\n",
    "result = vectorstore.similarity_search(f\"Search for: {search_term}\", k=5)\n",
    "for r in result:\n",
    "    nearest_embeddings_id = r.metadata[\"id\"]\n",
    "    nearest_cluster = df.loc[df['id'] == nearest_embeddings_id, \"cluster\"].iloc[0]\n",
    "    clusters.append(nearest_cluster)\n",
    "\n",
    "print(f\"Zu dem Suchbegriff '{search_term}' passt Cluster {','.join([str(x) for x in set(clusters)])} am besten\")\n",
    "\n",
    "paths = []\n",
    "for c in clusters:\n",
    "    paths.extend(df.loc[df['cluster'] == c, \"source\"].unique().tolist())\n",
    "\n",
    "print()\n",
    "print(\"\\n\".join(set(paths)))\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
